Class 6: AI Red Team
====================

..  image:: ./_static/mission6.png


.. NOTE::
       AI governance itself is a huge topic. Hence, we will only focus on the technical controls such as AI Red Team.


.. NOTE::
       No hands on lab for this class partly due to extensive resources required for scanning an AI apps / LLM. Demo and discussion will be conducted in class.
       


What is AI Governance?
~~~~~~~~~~~~~~~~~~~~~~
AI governance refers to the **frameworks**, **policies**, and **practices** established to guide the **ethical development, deployment, and use of artificial intelligence systems**. Its goal is to ensure that AI technologies are aligned with societal values, legal requirements, and organizational objectives while mitigating risks and addressing challenges related to bias, transparency, respect human rights, accountability, and security.



AI Risk/Safety vs AI Security
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**AI Safety/Risk**

Focus on preventing unintended harm caused by the AI services, ensuring it operates ethically and aligns with human values, considering broder societal impacts beyond just technical vulnerabiliites. - Protecting human using AI. (responsible use of AI and adherence to ethical standards). There are standard like NIST AI 600-1 that covered the Risk Management Framework for the AI Safety.


**AI Security**

Focus to secure AI services from external threats like cyberattack and data breach, safeguarding the confidentiality and integrity of the AI services. General framework that address this is OWASP Top 10 for LLM, MITRE ATLAS ( Adversarial Threat Landscape for Artificial Intelligence System) - Protecting AI system use by human.



What is F5 AI Red Team?
~~~~~~~~~~~~~~~~~~~~~~~

F5 AI Red Team tool is designed to help organizations proactively test and secure their AI applications, models and agents. It enables teams to simulate adversarial attacks, such as prompt injections and jailbreaks - at high scale and speed, to uncover potential risks, vulnerabilities in AI systems. It's similar to cybersecurity red teaming - defensive (Red) vs offensive (Blue), but focused specifically on finding ways AI apps might breaks or compromised.

Key offering includes:-

- A large and constantly updated threat-library and attack-agent swarm for testing existing and emerging AI vulnerabilities
- Continuous assessment capabilities so defenses evolve as AI threats evolve, across pilot to production environments
- Real-world scenario testing where it leverage 10,000+ monthly attack prompts and detailed “agentic fingerprint” insights (exploit paths, chain of reasoning)



..  image:: ./_static/class6-redteam-1.png

..  image:: ./_static/class6-redteam-2.png

..  image:: ./_static/class6-redteam-3.png

..  image:: ./_static/class6-redteam-4.png

..  image:: ./_static/class6-redteam-4-1.png



|
|

..  image:: ./_static/mission6-1.png

.. toctree::
   :maxdepth: 1
   :glob:

